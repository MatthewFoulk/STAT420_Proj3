---
title: "Hypothesis Tests, Confidence Intervals, Linear Regression"
author: "Student_Name/University ID"
output:
  pdf_document:
    toc: yes
  html_document:
    number_sections: yes
    toc: yes  
editor_options: 
  markdown: 
    wrap: 72
---

# Power Functions for Hypothesis Tests

## $H_0 : \mu \le \mu_0$ and $H_1: \mu > \mu_0$

Suppose the population has the normal distribution, and $\mu_0 = 5$ and
$\sigma^2 = 1.2$. Write a function 'power_1' that takes input: $n$ the
sample size, and $\mu$ - and returns the power function for a size
$\alpha = 0.05$ hypothesis test.

```{r}
power_1 <- function(n, x){
  sigma = sqrt(1.2)
  mu_0 <- 5
  alpha <- 0.05
  zalpha <- qnorm(p=alpha, lower.tail=FALSE)
  return (1 - pnorm(zalpha + sqrt(n)*(mu_0 - x))*1/sigma)
}
```

Plot the graphs of the power function (as a function of $\mu$) for the
following values of $n$: 1 (red), 2(purple), 3 (blue), 5 (brown), 10
(green), 50 (black). All the graphs must be on the same plot.

```{r}
x <-seq(from = 0, to = 10, length.out = 10000)
plot(x, power_1(1,x), type="l", col="red", xlab="Mu", ylab="Power", 
     main="Power Function")
lines(x, power_1(2,x), col="purple")
lines(x, power_1(3,x), col="blue")
lines(x, power_1(5,x), col="brown")
lines(x, power_1(10,x), col="green")
lines(x, power_1(50,x), col="black")
```

## $H_0 : \mu = \mu_0$ and $H_1: \mu \neq \mu_0$

Suppose the population has the normal distribution, and $\mu_0 = 5$ and
$\sigma^2 = 1.2$. Write a function 'power_2' that takes input: $n$ the
sample size, and $\mu$ - and returns the power function for a size
$\alpha = 0.05$ hypothesis test.

```{r}
power_2 <- function(n, x){
  sigma = sqrt(1.2)
  mu_0 <- 5
  alpha <- 0.05
  zalpha2 <- qnorm(p=alpha/2, lower.tail=FALSE) 
  pos <- pnorm(zalpha2 + sqrt(n)*(mu_0 - x)*(1/sigma))
  neg <- pnorm(-zalpha2 + sqrt(n)*(mu_0 - x)*(1/sigma))
  return (1-pos+neg)
}
```

Plot the graphs of the power function (as a function of $\mu$) for the
following values of $n$: 1 (red), 2(purple), 3 (blue), 5 (brown), 10
(green), 50 (black). All the graphs must be on the same plot.

```{r}
x <-seq(from = 0, to = 10, length.out = 10000)
plot(x, power_2(1,x), type="l", col="red", xlab="Mu", ylab="Power", 
     main="Power Function")
lines(x, power_2(2,x), col="purple")
lines(x, power_2(3,x), col="blue")
lines(x, power_2(5,x), col="brown")
lines(x, power_2(10,x), col="green")
lines(x, power_2(50,x), col="black")
```

```{r}
library(rlang)
library(dplyr)
```

# Confidence Intervals

The goal of this problem is to understand Confidence Intervals using
simulations. Recall that saying that you have a 95% confidence interval
means that if under repeated sampling, 95% of the confidence intervals
around the sample statistic will contain the population parameter.

## CI functions

Suppose population has the distribution $\text{Exp}(\lambda = 5)$. Use
the pivotal quantity coming from the MLE to calculate a 95% confidence
interval for $\lambda$. Call this function 'CI_Exp_Lambda'.

```{r}
CI_Exp_Lambda <- function(n){
  xbar = mean(rexp(n, 5))
  left = (qchisq(p=1-.05/2, df=2*n, lower.tail=TRUE))/(2*n*xbar)
  right = (qchisq(p=.05/2, df=2*n, lower.tail=TRUE))/(2*n*xbar)
  return(c(left, right))
}


```

Suppose population has the distribution $N(\mu = 10, \sigma^2 = 4)$. Use
the pivotal quantity coming from the MLE to calculate a 95% confidence
interval for $\mu^2$ (assume $\sigma^2$ is known).

```{r}
CI_Normal_Mu <- function(n){
  xbar = mean(rnorm(n, mean=10, sd=2))
  left = xbar - qnorm(p=.05/2)*2/sqrt(n)
  right = xbar + qnorm(p=.05/2)*2/sqrt(n)
  return(c(left, right))
}

```

## Simulations for Confidence Intervals

We will simulate the calculation of a 95% confidence interval for a
large number of samples of a fixed size from a population distribution
whose mean we know. We will then count the number of intervals that
contain the population mean.

Use the replicate function to run a simulation of the following
experiment: <l>

<li>set seed equal to 2022</li>

<li>calculate 100000 samples of size 50 from the population
distribution: exponential with rate parameter $\lambda = 4$.</li>

<li>use the mean_inside_CI function to check if the mean lies inside the
confidence interval, and store this result in the vector
'CI_95_exp'.</li>

<li>caculate the mean of 'CI_95_exp' to get the expected number of
confidence intervals that contain the population mean.</li>

</l> What do you notice?
```{r}
margin_error_95 <- function(sample_vals, sig){
    n <- length(sample_vals)
    mar_err <- 1.96*(sig/sqrt(n))
}
```

```{r}
mean_inside_CI <- function(samp, pop_mu, pop_sig){
    upper <- mean(samp) + margin_error_95(samp, pop_sig)
    lower <- mean(samp) - margin_error_95(samp, pop_sig)
    return(between(pop_mu, lower, upper))
}
```

```{r}
 set.seed(2022)
 B <- 100000
 n <- 50
 CI_95_exp <- replicate(B,{
  samp <- rexp(n,4)
  mean_inside_CI(samp, pop_mu = 1/4 , pop_sig = 1/4)
  }) 
                            
 print(mean(CI_95_exp)) #calculate mean CI_95_exp to get empirical expected values

```
What do you notice?
- We notice that the value is very close to 95.

Use the replicate function to run a simulation of the following
experiment: <l>

<li>set seed equal to 2020</li>

<li>calculate 100000 samples of size 50 from the population
distribution: $\text{Exp}(\lambda)$</li>

<li>write a function meansq_inside_CI function to check if $\mu^2$ lies
inside the confidence interval coming from the pivotal quantity obtained
from the distribution of the MLE, and store this result in the vector
'CI_95_meansq'.</li>

<li>caculate the mean of 'CI_95_meansq' to get the expected number of
confidence intervals that contain the population parameter $\mu^2$</li>

</l> What do you notice?
```{r}
mu2_me_95 <- function(sample_vals, sig){
    n <- length(sample_vals)
    xbar <- mean(sample_vals)
    mar_err <- 1.96*(2*sig*xbar/sqrt(n))
}
```

```{r}
meansq_inside_CI <- function(samp, pop_mu, pop_sig){
    upper <- mean(samp)^2 + mu2_me_95(samp, pop_sig)
    lower <- mean(samp)^2 - mu2_me_95(samp, pop_sig)
    return(between(pop_mu, lower, upper))
}
```

```{r}
set.seed(2020)
n <- 50
B <- 100000
CI_95_exp2 <- replicate(B, {
    samp <- rexp(n,rate=4)})

CI_95_meansq = vec()

meansq_inside_CI <- function(n){
  
}

```

Use the replicate function to run a simulation of the following
experiment: <l>

<li>set seed equal to 2020</li>

<li>calculate 100000 samples of size 50 from the population
distribution: $N(\mu = 10, \sigma = 4)$</li>

<li>write a function lambda_inside_CI function to check if lambda lies
inside the confidence interval coming from the pivotal quantity obtained
from the distribution of the MLE, and store this result in the vector
'CI_95_lambda'.</li>

<li>caculate the mean of 'CI_95_lambda' to get the expected number of
confidence intervals that contain the population parameter
$\lambda$</li>

</l> What do you notice?

```{r}
mu2_me_95 <- function(sample_vals, sig){
    n <- length(sample_vals)
    xbar <- mean(sample_vals)
    mar_err <- 1.96*(2*sig*xbar/sqrt(n))
}
```

```{r}
meansq_inside_CI <- function(samp, pop_mu, pop_sig){
    upper <- mean(samp)^2 + mu2_me_95(samp, pop_sig)
    lower <- mean(samp)^2 - mu2_me_95(samp, pop_sig)
    return(between(pop_mu^2, lower, upper))
}
```

```{r}
set.seed(2020)
n <- 50
B <- 100000
CI_95_meansq <- replicate(B, {
    samp <- rnorm(n,mean=10,sd=4)
    meansq_inside_CI(samp, pop_mu = 10, pop_sig = 4)
    })

print(mean(CI_95_meansq))
```
What do you notice?
- We notice that it is almost exactly 95%

## Visualizing confidence intervals when variance is known.

In this section we will visualize confidence intervals. Recall that
"calculating a 95% confidence interval" means: under repeated sampling
95% of the confidence intervals around the sample mean will contain the
population mean.

The following function "plot_CI_95" plots 95%-confidence intervals about
100 sample means coming from samples of size 30 and a normal population
distribution with mean 5 and stadard deviation 1.2. <l>

<li>The function takes input a random seed value and outputs a bunch of
confidence intervals.</li>

<li>The confidence intervals that do not contain the population mean are
plotted in red.</li>

<li>The sample means are plotted as black dots in the middle of the
confidence interval.</li>

<li>The population mean is identified by a blue vertical line (note that
the red confidence intervals do not intersect this line).</li>

</l>

```{r}
CI_95 <- function(samp, sig){
  z <- qnorm(0.025, lower.tail = FALSE)
  upper <- mean(samp) + z*sig/sqrt(length(samp))
  lower <- mean(samp) - z*sig/sqrt(length(samp))
  return(c(lower, upper))
}
```

Get familiar with the function "plot_CI_95" by evaluating it for a few
different seed values.

```{r}
plot_CI_95 <- function(seed){
  B <- 100
  n <- 30
  mu <- 5
  sig <- 1.2
  set.seed(seed)
  #extract upper bound of CI's
  x_1 <- replicate(B,
                         {samp <- rnorm(n, mean = mu, sd = sig )
                         max(CI_95(samp, sig))
                         }
                         )
  #extract lower bound of CI's
  set.seed(seed)
  x_0 <- replicate(B,
                         {samp <- rnorm(n, mean = mu, sd = sig )
                         min(CI_95(samp, sig))
                         }
                         )
  set.seed(seed)
  means <- replicate(B, mean(rnorm(n, mean = mu, sd = sig)))
 
  plot(means, 1:B, 
       pch = 20,
      xlim = c(mu - sig, mu + sig), 
      ylim = c(0,B+1),
      xlab = "sample means",
      ylab = "index of the CI",
      main = paste(B, "Confidence intervals"))
  for (i in 1:B){
  if(between(mu, x_0[i], x_1[i])){
  segments(x_0[i], i, x_1[i], i, lwd = 2) #plot CI's that contain the mean in black
    }
  else
    {
    segments(x_0[i], i, x_1[i], i, col = "red", lwd = 2) #plot CI's that don't contain the mean in red
    }
    }
abline(v=mu, col = "blue") #plot a vertical line at the population mean
}

plot_CI_95(402)
```

Now use the "Plot_CI_95" function to plot at six different values of the
random seed. What do you observe?

```{r}
 seeds <- c(10, 80, 15, 1, 1000, 8918) #choose six different seed values
 par(mfrow = c(1,2))
 for (i in seeds){
   plot_CI_95(i)
 }
   
```
I notice that the number of confidence intervals out of 100 are consistently around 5 for different random seeds. This is expected since our confidence is 95%, which means we'd expect 5% of CI's to not contain the mean.

## Visualizing CI's for $\lambda$

Modify the above code to visualize the confidence intervals in the case
when we are calculating 95% confidence intervals for $\lambda$ when
sampling from a population that has distribution
$\text{Exp}(\lambda = 5)$. Do two experiments: (1) with samples of size 8, (2) samples of size 60. What do you notice?

```{r}
my_CI_95 <- function(samp, sig){
  z <- qnorm(0.025, lower.tail = FALSE)
  upper <- mean(samp) + z*sig/sqrt(length(samp))
  lower <- mean(samp) - z*sig/sqrt(length(samp))
  return(c(lower, upper))
}
my_plot_CI_95 <- function(seed, n){
  B <- 100
  n <- n
  mu <- 1/5
  sig <- 1/5
  set.seed(seed)
  #extract upper bound of CI's
  x_1 <- replicate(B,
                         {samp <- rexp(n, 1/mu)
                         max(my_CI_95(samp, sig))
                         }
                         )
  #extract lower bound of CI's
  set.seed(seed)
  x_0 <- replicate(B,
                         {samp <- rexp(n, 1/mu)
                         min(my_CI_95(samp, sig))
                         }
                         )
  set.seed(seed)
  means <- replicate(B, mean(rexp(n, mu)))
 
  plot(means, 1:B, 
       pch = 20,
      xlim = c(mu - sig, mu + sig), 
      ylim = c(0,B+1),
      xlab = "sample means",
      ylab = "index of the CI",
      main = paste(B, "Confidence intervals"))
  for (i in 1:B){
  if(between(mu, x_0[i], x_1[i])){
  segments(x_0[i], i, x_1[i], i, lwd = 2) #plot CI's that contain the mean in black
    }
  else
    {
    segments(x_0[i], i, x_1[i], i, col = "red", lwd = 2) #plot CI's that don't contain the mean in red
    }
    }
abline(v=1/mu, col = "blue") #plot a vertical line at the population mean
}

my_plot_CI_95(402, 8)
my_plot_CI_95(402, 60)

```
I noticed that the CI's for n = 60 are much smaller and precise than the CI's for n = 8.

## Visualizing CI's for $\mu^2$

Modify the above code to visualize the confidence intervals in the case
when we are calculating 95% confidence intervals for $\mu^2$ when
sampling from a population that has distribution
$N(\mu = 10, \sigma^2 = 4)$ Do two experiments: (1) with samples of size 8, (2) samples of size 60. What do you notice?

```{r}
my_CI_95 <- function(samp, sig){
  z <- qnorm(0.025, lower.tail = FALSE)
  upper <- (mean(samp))^2 + z*sig/sqrt(length(samp))
  lower <- (mean(samp))^2 - z*sig/sqrt(length(samp))
  return(c(lower, upper))
}
my_plot_CI_95 <- function(seed, n){
  B <- 100
  n <- n
  mu <- 10
  sig <- 2
  set.seed(seed)
  #extract upper bound of CI's
  x_1 <- replicate(B,
                         {samp <- rnorm(n, mu, sig)
                         max(my_CI_95(samp, sig))
                         }
                         )
  #extract lower bound of CI's
  set.seed(seed)
  x_0 <- replicate(B,
                         {samp <- rnorm(n, mu, sig)
                         min(my_CI_95(samp, sig))
                         }
                         )
  set.seed(seed)
  means <- replicate(B, (mean(rnorm(n, mu, sig))))
 
  plot(means^2, 1:B, 
       pch = 20,
      xlim = c(mu^2 - sig, mu^2 + sig), 
      ylim = c(0,B+1),
      xlab = "sample means",
      ylab = "index of the CI",
      main = paste(B, "Confidence intervals"))
  for (i in 1:B){
  if(between(mu^2, x_0[i], x_1[i])){
  segments(x_0[i], i, x_1[i], i, lwd = 2) #plot CI's that contain the mean in black
    }
  else
    {
    segments(x_0[i], i, x_1[i], i, col = "red", lwd = 2) #plot CI's that don't contain the mean in red
    }
    }
abline(v=mu^2, col = "blue") #plot a vertical line at the population mean
}

my_plot_CI_95(402, 8)
my_plot_CI_95(402, 60)

```
I notice that with larger n, the CI's are more accurate. 


# Linear Regression

Recall that random samples for simple regression look like
$\{(x_i, Y_i)\; | \;\;Y_i = \alpha + \beta x_i + \epsilon_i \}$. Note
that $\alpha, \beta$ and $\sigma$ are model parameters and
$\{\epsilon_i \;\;| \;\; \epsilon_i \sim N(0, \sigma^2), i = 1, 2, 3, \dots, n. \}$

## Setup the random samples

Suppose we are working with a simple linear model with parameters:
$$\alpha = 3, \beta = 5, \sigma^2 = 1.6$$

Construct the random variable that generates the sample data for our
model:

```{r}
alpha <- 3
beta <- 5
sig <- sqrt(1.6)

x_fixed <- round(seq(3, 8, length.out = 15), 2)
Y <- function(alpha, beta, sig){
  alpha + beta*x_fixed + rnorm(1,mean = 0,sd = sig)
}
#y_vals <-c()
#for (i in x_fixed){
#  y_vals <- c(y,rnorm(1,mean = alpha+beta*i,sd = sig))
#}
#Random Samples for Simple Linear Regression.
empirical_results <- Y(3,5,1.6)
#Assume the true regression line is y = 2+3x
true_line <- 2+3*x_fixed
```

## Define the statistics

Define the statistics that take input two vectors $x$ and $y$ and output
estimates for (1) the slope parameter, (2) the intercept parameter, (3)
the variance parameter, (4) the prediction for a given value of x.

```{r}
 a_hat <- function(y,x){
   #using formula derived in class: alpha_mle = Y_bar - B_hat * x_bar
   return (mean(y)-b_hat(y,x)*mean(x))
 }
 b_hat <- function(y,x){
   x_bar = mean(x)
   y_bar = mean(y)
   x_diff = x-x_bar
   y_diff = y-y_bar
   return (sum(x_diff*y_diff)/sum(x_diff^2))
 }
 v_hat <- function(y){
   y_diff = y-y_bar
   n = length(y)
   return (1/n)*sum(y_diff^2)
 }

Y_hat <- function(y, xvals,x){
  #epsilon = rnorm(1,0,v_hat(samp))
  return (a_hat(y,xvals) + b_hat(y,xvals)*x)#+epsilon
}
```

## Estimated regression lines

Plot the line regression line $y = 3+5x$, for $x$ in 'x_fixed'. In this
same plot, add 50 estimated regression lines coming from 50 different
samples for the linear regression model.

```{r}
true_line <- 3+5*x_fixed

plot(x_fixed, true_line, type = 'l', col = "red",ylim= c(20,60),xlim=c(3,8))
x_vals <- seq(2,8,length=15)
for (i in seq(1,50)){
  #y_vals <- replicate(15,Y(3,5,1.6))
  y_vals <-c()
  for (i in x_fixed){
    y_vals <- c(y,rnorm(1,mean = alpha+beta*i,sd = 1.6))
  }
  y_hat <- Y_hat(y_vals,x_fixed,x_vals)
  lines(x_vals,y_hat)
}
```

## Sampling Distribution of the Slope parameter

Empirically verify that the sampling distribution of the slope parameter
is a normal distribution.

```{r}
Y_emp <- replicate(Y(3,5,1.6),50)
a_hat
```

What is the empirical variance? Compare this with the theoretical
variance.
